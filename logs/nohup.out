Some weights of the model checkpoint at /home/ohagi_masaya/TransBasedCitEmb/pretrainedmodel/scibert_scivocab_uncased were not used when initializing PTBCN: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing PTBCN from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing PTBCN from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of PTBCN were not initialized from the model checkpoint at /home/ohagi_masaya/TransBasedCitEmb/pretrainedmodel/scibert_scivocab_uncased and are newly initialized: ['cls.predictions.decoder.bias', 'ent_lm_head.dense.weight', 'ent_lm_head.dense.bias', 'ent_lm_head.layer_norm.gamma', 'ent_lm_head.layer_norm.beta', 'ent_lm_head.decoder.weight', 'ent_embeddings.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
arguments
Namespace(MAX_LEN=256, WINDOW_SIZE=125, batch_size=16, dataset='AASC', epoch=5, final_layer='feedforward', frequency=5, loss_type='CrossEntropy', lr=5e-05, mask_type='tail', predict=True, pretrained_model='scibert', test_data='excluded', train=False, train_data='excluded')
----loading data done----
GPU OK
parameters of SciBERT has been loaded.
train start
train end
----loading data----
----converting data----
----making embeddings----
0 9.861759185791016
1000 18.274322438281448
2000 17.796329556309583
3000 17.374154571236158
4000 17.090323428325462
5000 16.81294978037691
6000 16.720845657903162
7000 16.523363380950325
8000 16.309161862556323
9000 16.23974776815769
10000 15.995596149044145
11000 15.551468866425044
12000 15.3376851425323
13000 15.190644884919408
14000 14.93366862589431
15000 14.5105383309678
16000 14.186701842894887
17000 13.902918011583319
18000 13.574732577430337
19000 13.269535457761172
20000 12.997826333671158
21000 12.748257077073013
22000 12.47137138891231
23000 12.22641846509352
24000 12.036397672105613
25000 11.852025515087993
26000 11.689479199167442
27000 11.542357697771228
28000 11.375361342390079
29000 11.222441400847822
30000 11.111474899071231
31000 11.001214547829235
32000 10.88438979896616
33000 10.763791856138809
34000 10.81739027383587
35000 11.075038337191444
36000 11.318877630616273
37000 11.576397446995003
38000 11.905960253346723
39000 12.264669840750337
----saving embeddings----
----loading embeddings----
